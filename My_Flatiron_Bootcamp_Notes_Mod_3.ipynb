{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "My Flatiron Bootcamp Notes - Mod 3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jirvingphd/my_data_science_notes/blob/master/My_Flatiron_Bootcamp_Notes_Mod_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ca6efxfBAFa2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.dropbox.com/s/fchpltm5rnwd5ce/Flatiron%20Logo%202Wordmark.png?raw=1\" width=100 >"
      ]
    },
    {
      "metadata": {
        "id": "Hb2EiLRm_Ykt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# My Flatiron Bootcamp Notes  - Mod 3\n",
        "- James M. Irving, Ph.D.\n",
        "- james.irving.phd@gmail.com\n",
        "- Repo:  https://github.com/jirvingphd/my_data_science_notes\n",
        "\n",
        "- Previous Notebook\n",
        "    - [My Flatiron Bootcamp Notes - Mod 1 & 2.ipynb](https://drive.google.com/file/d/1Kd4jD0pzpaN2bsR7EkrXsvUMRLwS5a3l/view?usp=sharing)\n",
        "    \n"
      ]
    },
    {
      "metadata": {
        "id": "HDZhQiyGCH51",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Time Series \n"
      ]
    },
    {
      "metadata": {
        "id": "YywI36W1IRs7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Time Series with Pandas\n",
        "- Converting to datetime, setting index\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas.core import datetools\n",
        "\n",
        "temp_data.Date = pd.to_datetime(temp_data.Date, format='%d/%m/%y')\n",
        "temp_data.set_index('Date', inplace = True)\n",
        "\n",
        "```\n",
        "- Downsampling or upsampling time series\n",
        "\n",
        "```python\n",
        "\n",
        "# Downsampling (to larger time unit):\n",
        "temp_monthly= temp_data.resample('MS') # MS = month start\n",
        "\n",
        "# Upsampling (to smaller time unit, may cause NaN\n",
        "temp_bidaily= temp_data.resample('12H').asfreq()\n",
        "\n",
        "# Fill in emppty time indices:\n",
        "temp_bidaily_fill= temp_data.resample('12H').ffill() # Forwards fill\n",
        "temp_bidaily_fill= temp_data.resample('12H').bfill() #Backwards fill\n",
        "\n",
        "```\n",
        "- Slicing time series\n",
        "\n",
        "\n",
        "```python\n",
        "temp_1985_onwards = temp_data['1985':]\n",
        "```\n",
        "\n",
        "- Plotting time series: (ts=time series dataframe)\n",
        "```python\n",
        "# Line plot\n",
        "ts.plot(subplots=True/False)\n",
        "# Dot plot\n",
        "ts.plot(style='.b')\n",
        "# Histogram\n",
        "ts.hist()\n",
        "# KDE\n",
        "ts.plot(kind='kde')\n",
        "# Box & Whiskers\n",
        "ts.boxplot()\n",
        "# Heat maps\n",
        "year_matrix = nyse_annual.T  # First must transpose.\n",
        "plt.matshow(year_matrix, interpolation=None, aspect='auto', cmap=plt.cm.Spectral_r)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### Types of Time Series Trends\n",
        "\n",
        "- Stationary vs Non-Stationary \n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/utn0m1ry9raefx0/Mean_nonstationary.png?raw=1\" width=400>\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/d5o899hhus5ppxx/Var_nonstationary.png?raw=1\" width=400>\n",
        "\n",
        "- Trends can be:\n",
        "    - Linear\n",
        "    - Exponential\n",
        "    - Periodic/seasonal\n",
        "    - Trends with Increasing/Decreasing Variance\n",
        "    \n",
        "<img src=\"https://www.dropbox.com/s/pfpygr22gnrdz6m/trendseasonal.png?raw=1\" width=500>\n",
        "\n",
        "#### Trend detection: Rolling statistics:\n",
        "\n",
        "    - Moving average/variance calculations using ```.rolling()```\n",
        "\n",
        "```python\n",
        "rolmean = ts.rolling(window = 8, center = False).mean()\n",
        "rolstd = ts.rolling(window = 8, center = False).std()\n",
        "fig = plt.figure(figsize=(12,7))\n",
        "orig = plt.plot(ts, color='blue',label='Original')\n",
        "mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
        "std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
        "```\n",
        "<img src=\"https://www.dropbox.com/s/n6pfycjt0jntk1l/index_38_0.png?raw=1\" width=400>\n",
        "\n",
        "#### Trend detection: Dickey Fuller Test\n",
        "- [adfuller from statsmodels.tsa.statstools](http://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.adfuller.html)\n",
        "- The Dickey Fuller Test null hypothesis is that the series is NOT stationary, so a significant result means that it IS stationary. \n",
        "\n",
        "```python\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "dftest = adfuller(ts)\n",
        "\n",
        "# Extract and display test results in a user friendly manner\n",
        "dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
        "for key,value in dftest[4].items():\n",
        "    dfoutput['Critical Value (%s)'%key] = value\n",
        "print(dftest)\n",
        "\n",
        "```\n",
        "\n",
        "#### def stationarity_check(): from lessons\n",
        "```python\n",
        "def stationarity_check(TS):\n",
        "    \n",
        "    # Import adfuller\n",
        "    from statsmodels.tsa.stattools import adfuller\n",
        "    \n",
        "    # Calculate rolling statistics\n",
        "    rolmean = TS.rolling(window = 8, center = False).mean()\n",
        "    rolstd = TS.rolling(window = 8, center = False).std()\n",
        "    \n",
        "    # Perform the Dickey Fuller Test\n",
        "    dftest = adfuller(TS['#Passengers']) # change the passengers column as required \n",
        "    \n",
        "    #Plot rolling statistics:\n",
        "    fig = plt.figure(figsize=(12,6))\n",
        "    orig = plt.plot(TS, color='blue',label='Original')\n",
        "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
        "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Rolling Mean & Standard Deviation')\n",
        "    plt.show(block=False)\n",
        "    \n",
        "    # Print Dickey-Fuller test results\n",
        "    print ('Results of Dickey-Fuller Test:')\n",
        "\n",
        "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
        "    for key,value in dftest[4].items():\n",
        "        dfoutput['Critical Value (%s)'%key] = value\n",
        "    print (dfoutput)\n",
        "    \n",
        "    return None\n",
        "```\n",
        "- [Article on testing for non-stationary](https://machinelearningmastery.com/time-series-data-stationary-python/)\n",
        "\n",
        "#### Eliminating trends\n",
        "Have several methods for elimianting different trends:\n",
        "\n",
        "- **Taking the log transformation (or square root, cube root)**\n",
        "        - ```np.log(ts) or np.sqrt(ts) ```\n",
        "    - Will make time series more \"uniform\" over time. \n",
        "    - Higher values are penalized more than lower ones. \n",
        "- **Subtracting the rolling mean**\n",
        "    - Calculate the rolling mean ( using.rolling() ) and subtract it from the ts.\n",
        "\n",
        "```python\n",
        "\n",
        "rolmean = ts.rolling(window = 4).mean()\n",
        "ts_diff = ts - rolmean\n",
        "\n",
        "```\n",
        "        \n",
        "- **Weighted rolling mean.**\n",
        "    - Pandas has Exponentially Weighted Moving Average (ts.ewm())\n",
        "    - Halflife parameter determines exponentail decay. Can use other parameters like span and center of mass to define decay. \n",
        "        - Discussed in [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html)\n",
        "        \n",
        "```python\n",
        "# Use Pandas ewma() to calculate Weighted Moving Average of ts_log\n",
        "exp_rolmean = ts.ewm(halflife = 2).mean()\n",
        "data_minus_exp_rolmean = ts - exp_rolmean\n",
        "\n",
        "```\n",
        "\n",
        "- **Differencing**\n",
        "    - Common way dealing with both trends and seasonality is differencing.\n",
        "    - Take the difference between one instant and the previous instant (1-period /first order lag). # of time periods lag = the 'order' of diff. First, second, third, etc. \n",
        "    \n",
        "```python\n",
        "data_diff = data.diff(periods=365)\n",
        "```\n",
        "\n",
        "### Time Series Decomposition\n",
        "- Turns a time series into multiple different time series. Most often in 3 parts:\n",
        "    1. Seasonal \n",
        "    2. Trend\n",
        "    3. Random (noise/irregular/remainder/residuals)\n",
        "    \n",
        "- Must pick between addititve or multiplicative decomposition:\n",
        "    - Must analzye time series to help decide:\n",
        "        - Does the magnitude of seasonality increase or decrease when the time series increases?\n",
        "    - Statsmodels has seasonal_decompose function. \n",
        "\n",
        "```python\n",
        "# import seasonal_decompose\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "decomposition = seasonal_decompose(np.log(ts))\n",
        "\n",
        "# Gather the trend, seasonality and noise of decomposed object\n",
        "trend = decomposition.trend\n",
        "seasonal = decomposition.seasonal\n",
        "residual = decomposition.resid\n",
        "\n",
        "# Plot gathered statistics\n",
        " plt.plot(np.log(ts), label='Original', color=\"blue\")\n",
        "plt.plot(trend, label='Trend', color=\"blue\")\n",
        "plt.plot(seasonal,label='Seasonality', color=\"blue\")\n",
        "plt.plot(residual, label='Residuals', color=\"blue\")\n",
        "```\n",
        "<img src=\"https://www.dropbox.com/s/6dh8ogkytzjreky/index_4_0.png?raw=1\" width=500>\n",
        "\n",
        "\n",
        "- Article on [decomposing time series](https://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/)\n",
        "\n",
        "\n",
        "### Section 25 Recap\n",
        "The key takeaways from this section include:\n",
        "* When you import time series data into Pandas, make sure to use the time/date information as index values using either a Pandas Timestamp or Python DateTime data type\n",
        "* There are a range of built in functions in Pandas for easily downsampling or upsampling time series data\n",
        "* Line plots and dot plots can be useful for getting a sense of how a time series data set has changed over time\n",
        "* Histograms and density plots can be useful for getting a sense of the time independent distribution of a time series data set\n",
        "* Box and whisker plots per year (or other seasonality period - day, week, month, etc) can be a great way to easily see trends in the distribution of time series data over time\n",
        "* Heat maps can also be useful for comparing changes of time series data across a couple of dimensions. For example, with months on one axis and years on another they can be a great way to see both seasonality and year on year trends\n",
        "* A time series is said to be stationary if its statistical properties such as mean and variance remain constant over time\n",
        "* Most time series models work on the assumption that the time series are stationary (assumption of homoscedasticity)\n",
        "* Many time series data sets *do* have trends, violating the assumption of homoscedasticity\n",
        "* Common examples are trends include linear (straight line over time), exponential and periodic. Some data sets also have increasing (or decreasing) variance over time\n",
        "* Any given data set may exhibit multiple trends (e.g. linear, periodic and reduction in variance)\n",
        "* Rolling statistics can be used to test for trends to see whether the centrality and/or dispersion of the data set changes over time\n",
        "* The Dickey Fuller Test is a common test for determining whether a data set contains trends\n",
        "* Common approaches for removing trends and seasonality include taking a log transform,. subtracting the rolling mean and differencing\n",
        "* Decomposing allows you to separately view seasonality (which could be daily, weekly, annual, etc), trend and \"random\" which is the variability in the data set after removing the effects of the seasonality and trend\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "V96juU_9NarQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Time Series Models\n",
        "- Note: for almost all models you need to make time series stationary first.\n",
        "### White Noise Model\n",
        "- The white noise model has three properties:\n",
        "    - Fixed and constant mean\n",
        "    - Fixed and constant variance\n",
        "    - No correlation over time \n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/jk1pf891qfs2l4d/index_10_0.png?raw=1\" width=400>\n",
        "    \n",
        "- Special case is Gaussian White Noise\n",
        "    - Constant mean = 0\n",
        "    - Constant variance =1\n",
        "- [Article on white noise series in python](https://machinelearningmastery.com/white-noise-time-series-python/)\n",
        "\n",
        "### Random walk model\n",
        "- Very common in finance (i.e. exchange rates) \n",
        "    - Tomorrow's rate is heavily influenced by today's\n",
        "- Contrary to the white noise model, random walk has:\n",
        "    - No specific mean or variance.\n",
        "    - A strong dependence over time. \n",
        "\n",
        "- the changes over time are basically a white noise model \n",
        "\n",
        "$$Y_t = Y_{t-1} + \\epsilon_t$$\n",
        "where $\\epsilon_t$ is a *mean zero* white noise model!\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/cnlyxoos54ztlbx/index_12_0.png?raw=1\" width=400>\n",
        "\n",
        "\n",
        "#### Random Walk with a drift\n",
        "- The drift (c) steers the model in a certain direction.\n",
        "$$Y_t = c+ Y_{t-1} + \\epsilon_t$$\n",
        "\n",
        "\n",
        "### Correlation & Autocorrelation\n",
        "\n",
        "- [Article: \"A Gentle Introduction to Autocorrelation and Partial Autocorrelations\"](https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/\n",
        ")\n",
        "#### Autocorrelation Function (ACF)\n",
        "- Autocorrelation exmaines a time series against itself over increasing values of lag.\n",
        "- Pandas has autocorrelation_plot\n",
        "\n",
        "```\n",
        "pd.plotting.autocorrelation_plot(diet)\n",
        "```\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/e2uknvwydcijqnl/index_33_0%20%282%29.png?raw=1\" width=500 >\n",
        "\n",
        "- Same data, but after removing trends with differencing:\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/88u9r2cvnqrob2n/index_37_1.png?raw=1\" width=500>\n",
        "\n",
        "- Can also plot with statsmodels:.\n",
        "```python\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "plot_acf(diet, lags = 100);\n",
        "```\n",
        "<img src=\"https://www.dropbox.com/s/33llhqo96t8sh3j/index_45_0.png?raw=1\" width=500>\n",
        "#### Partial Autocorrelation Function (PACF)\n",
        "\n",
        "- Similar to ACF, but it controls for values at shorter labs (which ACF does not).\n",
        "    - \"Summary of tge relationship between a time series element and observations at a lab, _with the relationships of intervening observations removed_.\"\n",
        "    - Can be interpreted as a regression of the series against its PAST lags. \n",
        "    - Can use to help pick what order of ARF to use in modeling.\n",
        "\n",
        "Plotted from statsmodels tsaplots:\n",
        "\n",
        "```python\n",
        "from statsmodels.graphics.tsaplots import plot_pacf\n",
        "from matplotlib.pylab import rcParams\n",
        "plot_pacf(diet, lags = 100);\n",
        "\n",
        "```\n",
        "<img src=\"https://www.dropbox.com/s/cr7p0o3prwnmqrs/index_42_0.png?raw=1\" width=500>"
      ]
    },
    {
      "metadata": {
        "id": "hVHH24Z9IthJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ARMA Models\n",
        "- Combination of Autoregressive (AR) model and Moving Average (MA) model.\n",
        "    - AR: $Y_t = \\mu + \\phi * Y_{t-1}+\\epsilon_t$\n",
        "    - MA: $Y_t = \\mu +\\epsilon_t + \\theta * \\epsilon_{t-1}$\n",
        "\n",
        "#### The Autoregressive Model\n",
        "- A value from a time series is regressed on preivous values from same time series.\n",
        "\n",
        "$$ \\text{Today = constant + slope} \\times \\text{yesterday + noise} $$\n",
        "\n",
        "Or, mathematically:\n",
        "$$Y_t = \\mu + \\phi * Y_{t-1}+\\epsilon_t$$\n",
        "\n",
        "$\\phi$ is slope. \n",
        "\n",
        "\n",
        "- Notes on this formula:\n",
        "    - If the slope is 0, the ts is a white noise model with mean $\\mu$\n",
        "    - If slope is not 0, the ts is autocorrelated.\n",
        "    - Bigger slop means bigger autocorrelation\n",
        "    - Negative slope =  time series follows oscillatory process. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "VzZ0jE6_hdSI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### AR Model Time Series (at varying $\\phi$)\n",
        "**AR time series:**\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/k9mnam1wv4eltp2/AR_model.png?raw=1\" width =500>\n",
        "\n",
        "**AR series' ACF:**\n",
        "\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/5ucfnsrlxjev7k8/AR_ACF.png?raw=1\" width =500>\n",
        "> The oscillatory process of the time series with $\\phi=0.9$ is clearly reflected in the autocorrelation function, returning an oscillatory autocorrelation function as well. $\\phi=0.2$ leads to a very low, insignificant,  autocorrelation. $\\phi=0.8$ leads to a strong autocorrelation for the first few lags, and then incurs a steep decline. Having a $\\phi=1.02$ (just slightly bigger than 1) leads to strong and longlasting autocorrelation.\n",
        "\n",
        "\n",
        "**AR series' PACF:**\n",
        "\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/joazuyts1xmqhzh/AR_PACF.png?raw=1\" width=500>\n",
        "\n",
        "\n",
        "> For each of these PACFs, we notice a high value for 1 lag, then autocorrelations of 0, except for the second one. This is no big surprise, as the slope parameter is fairly small, so the relationship between a value and the next one is fairly limited."
      ]
    },
    {
      "metadata": {
        "id": "2Z4XoRF0i8wj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### The Moving Average Model\n",
        "- The weighted sum of today's and yesterday's noise \n",
        "\n",
        "$$ \\text{Today = Mean + Noise + Slope} \\times \\text{yesterday's noise} $$\n",
        "\n",
        "Or, mathematically:\n",
        "$$Y_t = \\mu +\\epsilon_t + \\theta * \\epsilon_{t-1}$$\n",
        "\n",
        "- Some notes based on this formula:\n",
        "    - If the slope is 0, the time series is a white noise model with mean $\\mu$\n",
        "    - If the slope is not 0, the time series is autocorrelated and depends on the previous white noise process\n",
        "    - Bigger slope means bigger autocorrelation\n",
        "    - When there is a negative slope, the time series follow an oscillatory process\n",
        "\n",
        "##### MA Model Time Series (at varying $\\phi$)\n",
        "**MA time series:**\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/ic7uzmgtuhdoqu4/MA_model.png?raw=1\"  width=500>\n",
        "\n",
        ">When there is a posivite $\\theta$ there is a certain persistence in level, meaning that each observation is generally close to its neighbors. This is more pronounced for higher . values of $\\theta$. MA series with negative coefficients, however, show oscillatory patterns. Recall that when $\\theta=0$, the process is a true White Noise Process! \n",
        "\n",
        "\n",
        "**MA ACF:**\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/fv7sryfxyazve82/MA_ACF.png?raw=1\" width=500>\n",
        "\n",
        "> MA processes have autocorrelations, but because of the structure of the MA formula (regressing it on the noise term of the previous observation) there is **only a dependence for one period, and the autocorrelation is zero for lags 2 and higher.**\n",
        "\n",
        "> If $\\theta >0$ the lag one autocorrelation is positive, if $\\theta <0$ the lag one autocorrelation is negative.\n",
        "\n",
        "\n",
        "**MA PACF:**\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/fsijauyvae9hj2v/MA_PACF.png?raw=1\" width=500>\n",
        "\n",
        "> Typically a strong correlation with the 1-period lag (strength depending in theta), and then the PACF gradually tails off. "
      ]
    },
    {
      "metadata": {
        "id": "yWGCsXhQkVlP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Higher Order AR(p) and MA(q) Models \n",
        "- First order:\n",
        "    - AR: $Y_t = \\mu + \\phi * Y_{t-1}+\\epsilon_t$\n",
        "    - MA: $Y_t = \\mu +\\epsilon_t + \\theta * \\epsilon_{t-1}$\n",
        "- Second order:\n",
        "    - AR(2): $Y_t = \\mu + \\phi_1 * Y_{t-1}+\\phi_2 * Y_{t-2}+\\epsilon_t$\n",
        "    - MA(2): $Y_t = \\mu +\\epsilon_t + \\theta_1 * \\epsilon_{t-1}+ \\theta_2 * \\epsilon_{t-2}$\n",
        "\n",
        "\n",
        "- AR(p):\n",
        "    - ACF for AR(p) would be strong until lag of p, then stagnant, then trail off. \n",
        "    - PACF for AR(p): Generally no correlation for lag values beyond p.\n",
        "- MA(q):\n",
        "    - ACF for MA(q) would show strong correlation up to a lag of q, the immedately delcine to minimal/no correction.\n",
        "    - PACF would show strong relationship to the lab and tailing off to no correlation afterwards.\n",
        "    \n",
        "    \n",
        "### ARMA Models:\n",
        "- In an ARMA model, is a regression on paste values (AR part) and the error term is modeled as a linear combo of error terms in the recent past (MA part). \n",
        "- Notation is generally ARMA(p,q)\n",
        "    - Example: ARMA(2,1) model equation\n",
        "     $$Y_t = \\mu + \\phi_1 Y_{t-1}+\\phi_2 Y_{t-2}+ \\theta \\epsilon_{t-1}+\\epsilon_t$$\n",
        "\n",
        "| | AR(p)   |   MA(q)  | ARMA(p,q)|\n",
        "|------|------|------|------|\n",
        "|   ACF | Tails off   |  Cuts off after lag q |  Tails off   |\n",
        "|   PACF | Cuts off after lag p  |   Tails off  |  Tails off  |\n",
        "\n",
        "\n",
        " #### General process when modeling with a time series:\n",
        "\n",
        "- Detrend your time series using differencing. ARMA models represent stationary processes, so we have to make sure there are no trends in our time series\n",
        "- Look at ACF and PACF of the time series\n",
        "- Decide on the AR, MA and order of these models\n",
        "- Fit the model to get the correct parameters and use for prediction\n",
        "\n",
        "\n",
        "[Additional Information on ARMA can be found here  in lessons 1 and 2.](https://newonlinecourses.science.psu.edu/stat510/node/41/)\n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "uVcxyBkymKe4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### sARIMA Models [BOOKMARK]\n",
        "- Integrated ARMA models. "
      ]
    },
    {
      "metadata": {
        "id": "eFY0lj9Um_MR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[BOOKMARK]"
      ]
    },
    {
      "metadata": {
        "id": "4B9BvEY_m_nY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Section 26: Key Takeaways\n",
        "\n",
        "The key takeaways from this section include:\n",
        "* A White Noise model has a fixed and constant mean and variance, and no correlation over time\n",
        "* A Random Walk model has no specified mean or variance, but has a strong dependance over time\n",
        "* The Pandas `corr()` function can be used to return the correlation between various time series data sets\n",
        "* Autocorrelation allows us to identify how strongly each time serties observation is related to previous observations\n",
        "*  The autocorrelation function (ACF) is a function that represents autocorrelation of a time series as a function of the time lag\n",
        "* The Partial Autocorrelation Function (or PACF) gives the partial correlation of a time series with its own lagged values, controlling for the values of the time series at all shorter lags\n",
        "* ARMA (AutoRegressive and Moving Average) modeling is a tool for forecasting time series values by regressing the variable on its own lagged (past) values\n",
        "* ARMA models assume that you've already detrended your data and that there is no seasonality\n",
        "* ARIMA (Integrated ARMA) models allow for detrending as part of the modeling process and work well for data sets with trends but no seasonality\n",
        "* SARIMA (Seasonal ARIMA) models allow for both detrending and seasonality as part of the modeling process\n",
        "* Fracebook Prophet enables data analysts and developers alike to perform forecasting at scale in Python\n",
        "* Prophet uses Additive Synthesis for time series forecasting\n"
      ]
    },
    {
      "metadata": {
        "id": "_L67aVcrnFEw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Distance Metrics & k-Nearest Neighbors\n",
        "### Distance Metrics:\n",
        "Distance helps us quantity similarity.\n",
        "Distance can be measured in different metrics.\n",
        "1. Manhattan Distance\n",
        "    - Movement by X/Y blocks.\n",
        "    - $d(x,y) =  \\sum_{k=1}^n |x_k - y_k|$\n",
        "        \n",
        "    <img src=\"https://www.dropbox.com/s/0q217qlbc9xtb7t/manhattan-distance.png?raw=1\" width=200>\n",
        "2. Euclidian/Pythagorean Distance\n",
        "    - Straight line (as-the-bird flies)\n",
        "    - $d(x,y) = \\sqrt{ \\sum_{k=1}^n  (x_k - y_k)^2)}$\n",
        "    \n",
        "    <img src=\"https://www.dropbox.com/s/h3ogtkukgp6pwin/euclidean-distance.png?raw=1\" width=250>\n",
        "    \n",
        "3. Minkowski distance\n",
        "    - Generalized distance metric across a _Normed Vector Space_. \n",
        "        - Meaning each point has been through the same function.  Can be any function as long as:\n",
        "            - A zero vector(just a vecotr of zeros) will output length=0\n",
        "            - Every other vector has positive length.\n",
        "        - Both Manhattan and Euclidian are actually special cases of Minkowski\n",
        "    - $d(p,q) = (\\sum_{i=1}^n (|p_i - q_i|)^c)^{1/c}$\n",
        "    \n",
        "    \n",
        "    \n",
        "```python \n",
        "# Manhattan Distance is the sum of all side lengths to the first power\n",
        "manhattan_distance = (length_side_1 + length_side2 + ... length_side_n)**1  \n",
        "\n",
        "# Euclidean Distance is the square root of the sum of all side lengths to the second power\n",
        "euclidean_distance = np.sqrt((length_side_1 + length_side2 + ... length_side_n)*2)\n",
        "\n",
        "# Minkowski Distance with a value of 3 would be the cube root of the sum of all side lengths to the third power\n",
        "minkowski_distance_3 = np.cbrt((length_side_1 + length_side2 + ... length_side_n)**3)\n",
        "\n",
        "# Minkowski Distance with a value of 5\n",
        "mink_distance_5 = np.power((length_side_1 + length_side2 + ... length_side_n)**5, 1./5)\n",
        "```\n",
        "\n",
        "\n",
        "### K-Nearest Neighbors (KNN)\n",
        "<img src=\"https://www.dropbox.com/s/77747858h369yzx/knn.gif?raw=1\" width=500>\n",
        "- **KNN is a supervised learning algorithm that can be used for both classification and regression.**\n",
        "    - Distance-based, looks for the smaller distance between 2 points to identify similarity. \n",
        "        - Each column acts as a dimension. \n",
        "        - Can use any of the distance metrics discussed\n",
        "    - since its supervised, must give it labeled training data. \n",
        "    \n",
        "- **Fitting**\n",
        "    - KNN does very little during the fit step, just stores the data and labels.\n",
        "- **Predicting**\n",
        "    - For each point, KNN calculates the distances to _every single point_ int he training set. \n",
        "    - It then finds the ```k``` closest neighbors, and examines their labels.\n",
        "        - its 'democratic', in that each of the nearest points submits a vote as to which group it should belong to.\n",
        "        - the group with the largest # of votes win. \n",
        "- **Evaluating Model Performance**\n",
        "    - Evaluation is different depending on if using for classification or regression task.\n",
        "    - Need a test set of data to compare its predicitons against to calc:\n",
        "        - Precision\n",
        "        - Recall\n",
        "        - Accuracy\n",
        "        - F1-Score"
      ]
    },
    {
      "metadata": {
        "id": "fagDue4O1_gC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Confusion Matrices - to Evaluate Classification\n",
        "For Example, using simply binary classification 0 or 1. \n",
        "<img src=\"https://www.dropbox.com/s/1kt3vniy7h1vodw/rf-conf-matrix.png?raw=1\" width=300>\n",
        "\n",
        "    \n",
        "- **Confusion Matrices tell us 4 things:**\n",
        "    - True Positives (TP): The model predicted the person has the disease (1), and they actually have the disease (1).\n",
        "\n",
        "    - True Negatives (TN): The model predicted the person is healthy (0), and they are actually healthy (0).\n",
        "\n",
        "    - False Positives (FP): The model predicted the person has the disease (1), but they are actually healthy (0). \n",
        "\n",
        "    - False Negatives (FN): The model predicted the person is healthy (0), but they actually have the disease (1).\n",
        "\n",
        "- **To construct a confusion matrix, we need:**\n",
        "    -  Predicitons for each data point in training or test set\n",
        "    - Labels for same data points in that test set.\n",
        "    \n",
        "- To create a Confusion Matrix from scratch, we:\n",
        "    1. Iterate through both lists and grab the item at the same the label and corresponding prediction.  \n",
        "        - Note that `enumerate` is great here, since it gives us both an item and the index of that item from a list. \n",
        "    2. Use some control flow to determine if its a TP, TN, FP, or FN. \n",
        "    3. Store our results in a dictionary or 2-dimensional array. \n",
        "    4. Return our results once we've checked every prediction against its corresponding label. \n",
        "    \n",
        "```python\n",
        "def confusion_matrix(labels, predictions):\n",
        "    conf_matrix = {\"TP\": 0, \"FP\": 0, \"TN\": 0, \"FN\": 0}\n",
        "    for ind, label in enumerate(labels):\n",
        "        pred = predictions[ind]\n",
        "        if label == 1:\n",
        "            # CASE: True Positive\n",
        "            if label == pred:\n",
        "                conf_matrix['TP'] += 1\n",
        "            # CASE: False Negative \n",
        "            else:\n",
        "                conf_matrix['FN'] += 1\n",
        "        else:\n",
        "            # CASE: True Negative\n",
        "            if label == pred:\n",
        "                conf_matrix['TN'] += 1\n",
        "            # CASE: False Positive\n",
        "            else:\n",
        "                conf_matrix['FP'] += 1\n",
        "    \n",
        "    return conf_matrix\n",
        "```\n",
        "\n",
        "- **Confusion Matrices for Multi-Categorical Classificaitons:**\n",
        "    - Diagonal represents true positives\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/qgy3t90fyxztjni/cm2.png?raw=1\" width=400>\n",
        "\n",
        "\n",
        "#### Confusion Matrices with sklearn\n",
        "- A nice positive of sklearn's implementation:\n",
        "    - it automatically adjusts to the# of categories present in the labels.\n",
        "    \n",
        "```python\n",
        "# Calcualate confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cf = confusion_matrix(example_labels, example_preds)\n",
        "\n",
        "# Plot confusion matrix with matplotlib\n",
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "def show_cf(y_true, y_pred, class_names=None, model_name=None):\n",
        "    cf = confusion_matrix(y_true, y_pred)\n",
        "    plt.imshow(cf, cmap=plt.cm.Blues)\n",
        "    \n",
        "    if model_name:\n",
        "        plt.title(\"Confusion Matrix: {}\".format(model_name))\n",
        "    else:\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    \n",
        "    class_names = set(y_true)\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    if class_names:\n",
        "        plt.xticks(tick_marks, class_names)\n",
        "        plt.yticks(tick_marks, class_names)\n",
        "    \n",
        "    thresh = cf.max() / 2.\n",
        "    \n",
        "    for i, j in itertools.product(range(cf.shape[0]), range(cf.shape[1])):\n",
        "        plt.text(j, i, cf[i, j], horizontalalignment='center', color='white' if cf[i, j] > thresh else 'black')\n",
        "\n",
        "    plt.colorbar()\n",
        "\n",
        "show_cf(example_labels, example_preds)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "nKjAFwz12Oeb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluation Metrics [BOOKMARK]\n",
        "- **Precision**\n",
        "$$Precision = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}}$$\n",
        "\n",
        "- **Recall**\n",
        "$$Recall = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}}$$ \n",
        "\n",
        " Precision and Recall have an inverse relationship.  As our recall goes up, our precision will go down, and vice versa. If this doesn't seem intuitive, let's examine this.\n",
        " \n",
        "<img src=\"https://www.dropbox.com/s/p7yy1t34lx9k82j/Precisionrecall.png?raw=1\" width=400>\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/ij75yic63m32x5z/performance-comparisons.png?raw=1\" width =400>\n",
        "\n",
        "\n",
        "- **Accuracy**\n",
        "\n",
        "$$Accuracy = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}}$$\n",
        "\n",
        "- **F-1 Score**\n",
        "\n",
        "$$F1-Score = 2\\ \\frac{Precision\\ x\\ Recall}{Precision + Recall}$$\n"
      ]
    },
    {
      "metadata": {
        "id": "r3ihBVCy_JYE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1D7nK5qU_M7X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Graph Theory\n",
        "\n",
        ">A \"Graph\" in mathematical and computer science terms consists of \"Nodes\" or \"Vertices\". Nodes/Vertices may or may not be connected with one another. The connecting line between two nodes is called an \"edge\". \n",
        "\n",
        "- Linked Refs:\n",
        "    - [Graph Theory Basics](https://www.geeksforgeeks.org/mathematics-graph-theory-basics-set-1/)\n",
        "    - [A Gentle Intro to Graph Theory](https://medium.com/basecs/a-gentle-introduction-to-graph-theory-77969829ead8)"
      ]
    },
    {
      "metadata": {
        "id": "-Eok_IyvDsEV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Graph Components  and Characteristics\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/s73t4jezg0wz6ml/Nodes%20and%20Edges.png?raw=1\" width =250>\n",
        "- **Basic Pieces of a Graph**\n",
        "    - __Node / Vertex__: The entity of analysis which has a relationship. \n",
        "        Node is used in the network context, vertex is used in the graph theory context, but commonly interchanged.\n",
        "\n",
        "    - __Link / Edge / Relationship__: The connections between the nodes.\n",
        "        Link is used in the network context, edge is used in the graph theory context, and all words are used interchangably with *relationship*.\n",
        "\n",
        "    - __Attributes__: Both nodes and edges can store attributes, which contain additional data about that object.\n",
        "\n",
        "    - __Weight__: A common *attribute* of edges, used to indicate *strength* or *value* of a relationship.\n",
        "\n",
        "- **Terminology**\n",
        "    - Adjacent Nodes:\n",
        "        - Node v is adjacent to node u if and only if there exists an edge between u and v.\n",
        "\n",
        "    - Path:\n",
        "        - A path of length n from node u to note v is defined as sequence of n+1 nodes.\n",
        "        $$P(u,v)=(v0,v1,v2,v3…….vn)$$\n",
        "\n",
        "        \n",
        "    - Degree of a node:\n",
        "        - In undirected graph\n",
        "            - A node's **degree** is the  # of nodes 'incident upon' the node. (AKA connected).\n",
        "        - In a directed graph,\n",
        "            - a node's **Indegree** is the # of arriving edges to the node\n",
        "            - **Outdegree** is the # of departing edges. \n",
        "    - Isolated Nodes\n",
        "        - Have no connection (degree=0)\n",
        "        - Isolated nodes cannot be found by _breadth first search_ (BFS)\n",
        "        \n",
        "<img src=\"https://www.dropbox.com/s/6ssw6smhwsntktw/deg.png?raw=1\" width=500>    \n",
        "\n",
        "- **Graph Thoery Summary**\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/g5sxt0udv7wkbck/summary.png?raw=1\" width=700>\n",
        "\n",
        "\n",
        "|                | Absent     | Present  |\n",
        "|----------------|------------|----------|\n",
        "| __Weights__ | Unweighted | Weighted |\n",
        "| __Directionality__ | Undirected | Directed |\n"
      ]
    },
    {
      "metadata": {
        "id": "enWz0R4Q_Pn1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I5SD1E9fIFUl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Graphs in Python with NetworkX\n"
      ]
    },
    {
      "metadata": {
        "id": "dRx-CF0lIH1S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}