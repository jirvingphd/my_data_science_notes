{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOW TO: DATA EXPLORATORY ANALYSIS & MUNGING \n",
    "--------------------------------------------\n",
    "\n",
    ">   Workflow from Pandas Lab.  \n",
    ">   *\\# DATA EXPLORATORY ANALYSIS*  \n",
    ">   **\\* Understanding the dimensionality of your dataset**\n",
    "\n",
    ">   **\\* Investigating what type of data it contains, and the data types used to\n",
    ">   store it**\n",
    "\n",
    ">   **\\* Discovering how missing values are encoded, and how many there are**\n",
    "\n",
    ">   **\\* Getting a feel for what information it does and doesnt contain**\n",
    "\n",
    ">   \\# Load in standard packages\n",
    "\n",
    ">   import pandas as pd  \n",
    ">   import numpy as np  \n",
    ">   import seaborn as sns  \n",
    ">   import matplotlib.pyplot as plt  \n",
    ">   %matplotlib inline\n",
    "\n",
    ">     \n",
    ">   *\\#1 Load the dataset and display header*  \n",
    ">   `import pandas as pd  \n",
    ">   df=pd.read_csv(‘Data.csv’)  \n",
    ">   df.head(2)`\n",
    "\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#2 Drop any unwanted Unnamed Index Columns\n",
    "df.drop(‘Unnamed: 0’, axis=1, inplace=True) #axis =1 to spec columns\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    ">   *\\#3 Use .info() to check and change datatypes*`  \n",
    ">   print('DF INFORMATION:\\n')  \n",
    ">   print(df.info())  \n",
    ">   print('\\n')  \n",
    ">     \n",
    ">   `*\\# Check for NaN Values, Missing, and Odd Categorical Values*  \n",
    ">   print(f'NaN values in data? \\\\n {heroes_df.isna().any().any()}\\\\n')  \n",
    ">   print(f'Num missing: {heroes_df.Publisher.isna().sum()} out of\n",
    ">   {len(heroes_df.Publisher)}, ({round((heroes_df.Publisher.isna().sum()\n",
    ">   /len(heroes_df.Publisher) \\* 100),2) }%)')  \n",
    ">   print('heroes_df - Missing Publisher values:'.upper())  \n",
    ">   print(f'{df.Publisher.isna().sum()/len(heroes_df.Publisher)\\*100}%')  \n",
    ">   print('\\\\n')\n",
    "\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Check for Check for Redundancy\n",
    "print(f'Num unique: {len(df.Categorical_Col.unique())}')\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# DEALING WITH NULL VALUES\n",
    "# Choices are 1) to drop the missing data (row or col)\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "[BOOKMARK]# \n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "print('   HEROES_DF INFORMATION:\\n')\n",
    "    print(heroes_df.info())\n",
    "    print('\\n')\n",
    "    # print('heroes_df value counts (check for redundancy):')\n",
    "    # print(heroes_df['name'].value_counts()[0:10])\n",
    "    # print('\\n\\n')\n",
    "\n",
    "    print(f'NaN values in data? \\n {heroes_df.isna().any().any()}\\n')\n",
    "\n",
    "    # print('heroes_df names - Number of Redundant:'.upper())\n",
    "    # print(f'Num unique names: {len(heroes_df.name.unique())}')\n",
    "    # print(f'Num of repeated names: {len(heroes_df.name)-len(heroes_df.name.unique())}')\n",
    "    # print('\\n')\n",
    "\n",
    "    print('heroes_df -  Missing Publisher values:'.upper())\n",
    "    #print(f'Num unique: {len(heroes_df.Publisher.unique())}')\n",
    "    print(f'Num missing: {heroes_df.Publisher.isna().sum()} out of {len(heroes_df.Publisher)}, ({round((heroes_df.Publisher.isna().sum() /len(heroes_df.Publisher) * 100),2) }%)')\n",
    "    # print(f'{heroes_df.Publisher.isna().sum()/len(heroes_df.Publisher)*100}%')\n",
    "    print('\\n')\n",
    "\n",
    "    print('heroes_df - Missing Weight values:'.upper())\n",
    "    # print(f'Num missing: {heroes_df.Weight.isna().sum()}')\n",
    "    print(f'Num missing: {heroes_df.Weight.isna().sum()} out of {len(heroes_df.Weight)}, ({round((heroes_df.Weight.isna().sum() /len(heroes_df.Weight) * 100),2) }%)')\n",
    "    print('\\n')–\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Multiple Linear Regression*\n",
    "============================\n",
    "\n",
    "\\* HOW TO: BLOG POST ON LINEAR REGRESSION IN PYTHON\n",
    "---------------------------------------------------\n",
    "\n",
    "<https://www.dropbox.com/s/bzg4o8ndtu70byg/Linear%20Regression%20in%20Python%20-%20Blog%20Post.pdf?dl=0>\n",
    "\n",
    "1.  Step 1: visualization\n",
    "\n",
    "    1.  Look for linear relationship – use Seaborn’s pairplot  \n",
    "        sns.pairplot(data, x_vars = [b1,b2,b3], y_vars=’Sales’,kind=’reg’)  \n",
    "        \\# Note, can also pass ‘size= “ for change plot size.  \n",
    "        \\# kind = ‘reg’ attempts to add line of best fit and 95% confidence\n",
    "        ivtervanl (will aim to minimize the sum of squared error)\n",
    "\n",
    "2.  Step 2: SK Learn – Setting Variables\n",
    "\n",
    "    1.  Scikit-Learn expects X to be a ‘feature matrix’ (Pandas DataFrame) and y\n",
    "        to be a ‘response vector’\n",
    "\n",
    "    2.  X=dataframe. y = y from the dataframe\n",
    "\n",
    "3.  Step 3: SK Learn – Splitting our data  \n",
    "    from sklearn.cross_validation import test_train_split  \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "4.  Step 4: SK Learn – Training our model  \n",
    "    \\# Import linear regression and instantiate  \n",
    "    from sklearn.linear_model import LinearRegression  \n",
    "    linreg = LinearRegression()  \n",
    "      \n",
    "    \\# Fit model to training data  \n",
    "    linred.fit(X_train, y_train)\n",
    "\n",
    "5.  Step 5: Interpreting Coefficients  \n",
    "    print(lingreg.intercept_) \\# prints y-intercept, BO  \n",
    "    print(linreg.coef_) \\# prints beta coeffiicents in same order as passed  \n",
    "    zip(feature_cols, linreg.coef_) \\# Pair feature names and coefficients\n",
    "\n",
    "6.  Step 6: Making predictions  \n",
    "    y_pred = linreg.predict(X_test)\n",
    "\n",
    "7.  Step 7: Model Evaluation  \n",
    "    from sklearn import metrics  \n",
    "      \n",
    "    \\# Most popular metric to use is root-mean-square-error (RMSE)  \n",
    "    print(np.sqrt(metrics.mean_squared_error(y_true, y_pred)))  \n",
    "      \n",
    "    \\#People also use Mean Absolute Error or Mean-Squared Error, but harder to\n",
    "    interpret\n",
    "\n",
    "8.  Step 8: Feature selection:\n",
    "\n",
    "    1.  Once have error metric, take note which X’s have minimal impact on y.\n",
    "\n",
    "        1.  Removing some of these may increase the accuracy of the model\n",
    "\n",
    "    2.  Now, process of trial and error, starting over again (dropping columns)\n",
    "        until reach a satisfactory model\n",
    "\n",
    "    3.  Recommended Steps\n",
    "\n",
    "        1.  Replace feature_cols & X\n",
    "\n",
    "        2.  Train_test_split your data\n",
    "\n",
    "        3.  Fit the model to linreg again using linreg.fit\n",
    "\n",
    "        4.  Make predictions using (y_pred = linreg.predict(X_test))\n",
    "\n",
    "        5.  Compute RMSE\n",
    "\n",
    "        6.  Repeat until RMSE satisfactory\n",
    "\n",
    "### Code from: FEATURE SCALING AND NORMALIZATION LAB:\n",
    "\n",
    "1.  Performing binning / as categories for numerical categorical variables for\n",
    "    regression, create dummy variables ( and replace orig):\n",
    "\n",
    ">   \\# first, create bins for based on the values observed. 5 values will result\n",
    ">   in 4 bins\n",
    "\n",
    ">   bins = [0, 3, 4 , 5, 24]\n",
    "\n",
    ">   bins_rad = pd.cut(boston_features['RAD'], bins)\n",
    "\n",
    ">   bins_rad = bins_rad.cat.as_unordered()\n",
    "\n",
    ">   \\# first, create bins for based on the values observed. 5 values will result\n",
    ">   in 4 bins\n",
    "\n",
    ">   bins = [0, 250, 300, 360, 460, 712]\n",
    "\n",
    ">   bins_tax = pd.cut(boston_features['TAX'], bins)\n",
    "\n",
    ">   bins_tax = bins_tax.cat.as_unordered()\n",
    "\n",
    ">   tax_dummy = pd.get_dummies(bins_tax, prefix=\"TAX\")\n",
    "\n",
    ">   rad_dummy = pd.get_dummies(bins_rad, prefix=\"RAD\")\n",
    "\n",
    ">   boston_features = boston_features.drop([\"RAD\",\"TAX\"], axis=1)\n",
    "\n",
    ">   boston_features = pd.concat([boston_features, rad_dummy, tax_dummy], axis=1)\n",
    "\n",
    ">   boston_features = boston_features.drop(\"NOX\",axis=1)\n",
    "\n",
    "1.  Filtering out the columns of a dataframe using drop, filter, and regex :\n",
    "\n",
    ">   df= boston_features\n",
    "\n",
    ">   boston_cont = df[df.columns.drop(list(df.filter(regex='TAX')))]\n",
    "\n",
    ">   boston_cont =\n",
    ">   boston_cont[boston_cont.columns.drop(list(boston_cont.filter(regex='RAD')))]\n",
    "\n",
    ">   boston_cont= boston_cont.drop(['CHAS'], axis=1)\n",
    "\n",
    "1.  Different Tpes of transformations on the dataframe:\n",
    "\n",
    ">   data_log = df_log\n",
    "\n",
    ">   age = boston_cont[\"AGE\"]\n",
    "\n",
    ">   b = boston_cont[\"B\"]\n",
    "\n",
    ">   rm = boston_cont[\"RM\"]\n",
    "\n",
    ">   logcrim = data_log[\"CRIM\"]\n",
    "\n",
    ">   logdis = data_log[\"DIS\"]\n",
    "\n",
    ">   logindus = data_log[\"INDUS\"]\n",
    "\n",
    ">   loglstat = data_log[\"LSTAT\"]\n",
    "\n",
    ">   logptratio = data_log[\"PTRATIO\"]\n",
    "\n",
    ">   features_final= pd.DataFrame([])\n",
    "\n",
    ">   features_final[\"CRIM\"] = (logcrim-min(logcrim))/(max(logcrim)-min(logcrim))\n",
    "\n",
    ">   features_final[\"B\"] = (b-min(b))/(max(b)-min(b))\n",
    "\n",
    ">   features_final[\"RM\"] = (rm-min(rm))/(max(rm)-min(rm))\n",
    "\n",
    ">   features_final[\"DIS\"] = (logdis-np.mean(logdis))/np.sqrt(np.var(logdis))\n",
    "\n",
    ">   features_final[\"INDUS\"] =\n",
    ">   (logindus-np.mean(logindus))/np.sqrt(np.var(logindus))\n",
    "\n",
    ">   features_final[\"LSTAT\"] =\n",
    ">   (loglstat-np.mean(loglstat))/(max(loglstat)-min(loglstat))\n",
    "\n",
    ">   features_final[\"AGE\"] = (age-np.mean(age))/(max(age)-min(age))\n",
    "\n",
    ">   features_final[\"PTRATIO\"] = (logptratio)/(np.linalg.norm(logptratio))\n",
    "\n",
    "### Code from: Regression modeling with Boston Housing Dataset\n",
    "\n",
    "![](media/05194d71a28f0a54e57e63ea704f485b.png)\n",
    "\n",
    ">   \\# Your code here\n",
    "\n",
    ">   import statsmodels.api as sm\n",
    "\n",
    ">   import statsmodels.formula.api as smf\n",
    "\n",
    ">   import scipy.stats as stats\n",
    "\n",
    ">   import statsmodels.stats.api as sms\n",
    "\n",
    ">   \\# The results will be saved in results list\n",
    "\n",
    ">   results = [['ind_var','r_sqared','intercept','slope','p-value','norm_JB']]\n",
    "\n",
    ">   *\\# TO LOOP THROUGH LIST OF DATACOLUMNS TO RUN OLS REGRESSION + PRINT/SAVE\n",
    ">   RESULTS*\n",
    "\n",
    ">   for idx, val in enumerate(['crim','dis','rm','zn','age']):\n",
    "\n",
    ">   print (\"Boston Housing DataSet - Regression Analysis and Diagnostics for\n",
    ">   formula: medv\\~\" + val)\n",
    "\n",
    ">   print\n",
    ">   (\"-------------------------------------------------------------------------------------\")\n",
    "\n",
    ">   f = 'medv\\~' + val\n",
    "\n",
    ">   model = smf.ols(formula=f,data=data).fit()\n",
    "\n",
    ">   X_new = pd.DataFrame({val: [data[val].min(),data[val].max()]})\n",
    "\n",
    ">   preds= model.predict(X_new)\n",
    "\n",
    ">   data.plot(kind='scatter',x=val,y='medv')\n",
    "\n",
    ">   plt.plot(X_new,preds,c='red',linewidth=2)\n",
    "\n",
    ">   plt.show()\n",
    "\n",
    ">   fig=plt.figure(figsize=(15,8))\n",
    "\n",
    ">   fig = sm.graphics.plot_regress_exog(model, val, fig=fig)\n",
    "\n",
    ">   fig = sm.graphics.qqplot(model.resid,dist=stats.norm, line='45',fit=True )\n",
    "\n",
    ">   plt.show\n",
    "\n",
    ">   results.append([val,model.rsquared,model.params[0],model.params[1],model.pvalues[1],sms.jarque_bera(model.resid)[0]])\n",
    "\n",
    ">   input('Press Enter to continue...')\n",
    "\n",
    "### Code from: Dealing with categorical variables lab\n",
    "\n",
    ">   *\\#Get list of column names (to use for plotting from df)*\n",
    "\n",
    ">   names = boston_df.columns\n",
    "\n",
    ">   nameList = [str(x) for x in names]\n",
    "\n",
    ">   col_names = nameList[1:]\n",
    "\n",
    ">   print(col_names)\n",
    "\n",
    "*\\# Loop through each column to plot*\n",
    "\n",
    ">   for col in col_names:\n",
    "\n",
    ">   plt.figure()\n",
    "\n",
    ">   plt.scatter(boston_df[col],boston_df['MEDV'],label=col,marker='.')\n",
    "\n",
    ">   plt.legend()\n",
    "\n",
    "### REGRESSION MODEL VALIDATION\n",
    "\n",
    "\\-using train-test-split\n",
    "\n",
    "![](media/480152d587e701dde0f1599c9d684426.png)\n",
    "\n",
    ">   *\\# Using train-test-split from sklearn*\n",
    "\n",
    ">   from sklearn.model_selection import train_test_split\n",
    "\n",
    ">   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    ">   from sklearn.linear_model import LinearRegression  \n",
    ">   lingreg=LinearRegression(X_train, y_train)\n",
    "\n",
    ">   y_hat_train = linreg.predict(X_train)  \n",
    ">   y_hat_test = linreg.predict(X_test)\n",
    "\n",
    ">   train_residuals = y_hat_train – y_train  \n",
    ">   test_residuals = y_hat_test – y_test\n",
    "\n",
    ">   mse_train = np.sum((y_train – y_hat_train)\\*\\*2/len(y_train)  \n",
    ">   mse_test = np.sum((y_test – y_hat_test)\\*\\*2/len(y_test)\n",
    "\n",
    "**Select columns using regex**\n",
    "------------------------------\n",
    "\n",
    "df.filter(regex=('Mark'),axis=1).describe()\n",
    "\n",
    "SECTION 12: COMPLETE PROJECT\n",
    "============================\n",
    "\n",
    "Modeling Our Data Lab/Lesson:\n",
    "-----------------------------\n",
    "\n",
    "-   Load in pre-cleaned file\n",
    "\n",
    "    -   This is after having cleaned the dataset and made dummy variables.\n",
    "\n",
    "    -   Must re-cast categories as categories when reloading data\n",
    "\n",
    "-   If there are a lot of possible predictors, should try starting with single\n",
    "    linear regressions (on CONTINUOUS)\n",
    "\n",
    "    -   Using statsmodels.formula.api as smf\n",
    "\n",
    "    >   import statsmodels.formula.api as smf  \n",
    "    >   \\# .describe used to select non-categorical values, then drop target var\n",
    "\n",
    "    >   col_names = dataframe.describe().columns.drop([Target_Var'])\n",
    "\n",
    "    >   results = [['ind_var', 'r_squared', 'intercept', 'slope', 'p-value' ]]\n",
    "\n",
    "    >   *\\# Use loop to run ols model with f=’Target_Variable\\~’+val*  \n",
    "    >   for idx, val in enumerate(col_names):\n",
    "\n",
    "    >   print (\"Walmart: Weekly_Sales\\~\" + val)\n",
    "\n",
    "    >   print (\"------------------------------\")\n",
    "\n",
    "    >   f = 'Weekly_Sales\\~' + val\n",
    "\n",
    "    >   model = smf.ols(formula=f, data=walmart).fit()\n",
    "\n",
    "    >   X_new = pd.DataFrame({val: [walmart[val].min(), walmart[val].max()]});\n",
    "\n",
    "    >   preds = model.predict(X_new)\n",
    "\n",
    "    >   results.append([val, model.rsquared, model.params[0], model.params[1],\n",
    "    >   model.pvalues[1] ])\n",
    "\n",
    "    >   print(results[idx+1])  \n",
    "    >   pd.DataFrame(results)\n",
    "\n",
    "-   Examine outputs:\n",
    "\n",
    "    -   What do the parameter estimates mean? Do they make sense?\n",
    "\n",
    "    -   What do the p-values tell us?\n",
    "\n",
    "    -   What does the R-squared tell us?\n",
    "\n",
    "-   If poor R-squared, re-examine distributions\n",
    "\n",
    "    -   Dataframe.hist()\n",
    "\n",
    "    -   If skewed data can log transform:\n",
    "\n",
    "        -   If negative data:\n",
    "\n",
    "            -   walmart_log= walmart[walmart[\"Weekly_Sales\"]\\>0]\n",
    "\n",
    "        -   walmart_log[\"Weekly_Sales\"]= np.log(walmart_log[\"Weekly_Sales\"])\n",
    "\n",
    "    -   Re-run loop from earlier:\n",
    "\n",
    "        -   compare and contract the results with the results obtained when we\n",
    "            did not take the log(sales)\n",
    "\n",
    "        -   Which one would you want to proceed with based on this?\n",
    "\n",
    "-   Build a model with each category variable as a predictor (can re-run data vs\n",
    "    data-log, re-examine the R-square output)\n",
    "\n",
    "    -   Put all categories for one categorical variable in 1 model (so 4 models\n",
    "        if 4 different categorical variables\n",
    "\n",
    "        -   IF USED DUMMY CODES, MUST DROP 1 FOR BETTER RESULTS (not explained)\n",
    "\n",
    "    -   Use output to judge choice of data vs data_log.\n",
    "\n",
    "-   **Use the model results to identify variables that we can drop from the\n",
    "    model.**\n",
    "\n",
    "    -   Can do manually (drop from dataframe and re-run)\n",
    "\n",
    "    -   **Can Use RECURSIVE FEATURE ELIMINATION FOR X NUMBER OF FEATURES**\n",
    "\n",
    "        -   Create a for loop (below is 5-\\>85 by 10’s)\n",
    "\n",
    "    >   from sklearn.feature_selection import RFE\n",
    "\n",
    "    >   from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    >   linreg = LinearRegression()\n",
    "\n",
    "    >   r_list = []\n",
    "\n",
    "    >   adj_r_list = []\n",
    "\n",
    "    >   list_n = list(range(5,86,10))\n",
    "\n",
    "    >   for n in list_n:\n",
    "\n",
    "    >   select_n = RFE(linreg, n_features_to_select = n)\n",
    "\n",
    "    >   select_n = select_n.fit(X, np.ravel(y))\n",
    "\n",
    "    >   selected_columns = X.columns[select_n.support\\_ ]\n",
    "\n",
    "    >   linreg.fit(X[selected_columns],y)\n",
    "\n",
    "    >   yhat = linreg.predict(X[selected_columns])\n",
    "\n",
    "    >   SS_Residual = np.sum((y-yhat)\\*\\*2)\n",
    "\n",
    "    >   SS_Total = np.sum((y-np.mean(y))\\*\\*2)\n",
    "\n",
    "    >   r_squared = 1 - (float(SS_Residual))/SS_Total\n",
    "\n",
    "    >   print(r_squared)\n",
    "\n",
    "    >   adjusted_r_squared = 1 - (1-r_squared)\\*(len(y)-1)/(len(y)-X.shape[1]-1)\n",
    "\n",
    "    >   print(adjusted_r_squared)\n",
    "\n",
    "    >   r_list.append(r_squared)\n",
    "\n",
    "    >   adj_r_list.append(adjusted_r_squared)\n",
    "\n",
    "“What we see is that both MSE keeps improving when we add variables. It seems\n",
    "like a bigger model improves our performance, and the test and train performance\n",
    "don't really diverge. It is important to note however that is not an unusual\n",
    "result. The performance measures used typically will show this type of behavior.\n",
    "In order to really be able to balance the curse of dimensionality (which will\n",
    "become more important in machine learning), we need other information criteria\n",
    "such as AIC and BIC. You'll learn about them later! Now, let's perform\n",
    "cross-validation on our model with 85 predictors!”\n",
    "\n",
    "-   Can do a 10-fold cross validation with the final model.\n",
    "\n",
    "    >   from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    >   from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    >   \\# select 85 best predictors\n",
    "\n",
    "    >   select_85 = RFE(linreg, n_features_to_select = 85)\n",
    "\n",
    "    >   select_85 = select_n.fit(X, np.ravel(y))\n",
    "\n",
    "    >   selected_columns = X.columns[select_n.support_]\n",
    "\n",
    "    >   cv_10_results = cross_val_score(linreg, X[selected_columns], y, cv=10,\n",
    "    >   scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "    >   cv_10_results\n",
    "\n",
    "“Running our 10-fold cross-validation highlights some issues for sure! Have a\n",
    "look at your list of 10 MSEs. Where most MSEs are manageable, some are very\n",
    "high. The cure of dimensionality is already pretty clear here. The issue is that\n",
    "we have many (dummy) categorical variables that result in columns with many\n",
    "zeroes and few ones. This means that for some folds, there is a risk of ending\n",
    "up with columns that almost exclusively contain 0's for prediction, which might\n",
    "cause weird results. Looking at this, a model with less predictors might make\n",
    "sense again. This is where we conclude for now. It's up to you now to explore\n",
    "other model options! Additionally, it is encouraged to try some of the \"level\n",
    "up\" exercises below. Good luck!”\n",
    "\n",
    "Data Science Project Workflow Notes (section12)\n",
    "===============================================\n",
    "\n",
    "Order of Processing (using OSEMN model)\n",
    "=======================================\n",
    "\n",
    "1.  **OBTAIN: Import data, inspect, check for datatypes to convert and null\n",
    "    values**\n",
    "\n",
    "    -   Display header and info\n",
    "\n",
    "    -   Drop any unneeded columns (df.drop(['col1','col2'],axis=1)\n",
    "\n",
    "2.  **SCRUB: cast data types, identify outliers, check for multicollinearity,\n",
    "    normalize data**\n",
    "\n",
    "    -   Check and cast data types\n",
    "\n",
    "        -    Check for \\#'s that are store as objects (df.info())\n",
    "\n",
    "            -   when converting to \\#'s, look for odd values (like many 0's), or\n",
    "                strings that can't be converted\n",
    "\n",
    "            -   Decide how to deal weird/null values (df.unique(),\n",
    "                df.isna().sum(), df.describe()-min/max, etc\n",
    "\n",
    "        -    Check for categorical variables stored as integers (for now cast as\n",
    "            strings)\n",
    "\n",
    "    -   Check for missing values (df.isna().sum())\n",
    "\n",
    "        -   Can drop rows or colums\n",
    "\n",
    "        -   For missing numeric data with median or bin/convert to categorical\n",
    "\n",
    "        -   For missing categorical data: make NaN own category OR replace with\n",
    "            most common category\n",
    "\n",
    "    -   Check for multicollinearity\n",
    "\n",
    "        -   use seaborn to make correlation matrix plot [Evernote\n",
    "            Link](https://www.evernote.com/l/AArNyaEwjA5JUL6I9PazHs_ts_hU-m7ja1I/)\n",
    "\n",
    "            -   Good rule of thumb is anything over 0.75 corr is high, remove\n",
    "                the variable that has the most correl with the largest \\# of\n",
    "                variables\n",
    "\n",
    "    -   Normalize data (may want to do after some exploring)\n",
    "\n",
    "        -   Most popular is Z-scoring (but won't fix skew)\n",
    "\n",
    "        -   Can log-transform to fix skewed data\n",
    "\n",
    "3.  **EXPLORE: Check distributions, outliers, etc**\n",
    "\n",
    "    -   Check scales, ranges (df.describe())\n",
    "\n",
    "    -   Use histograms to get an idea of distribut(df.hist())\n",
    "\n",
    "        -   Can also do kernel density estimates\n",
    "\n",
    "    -    use scatterplots to check for linearity and possible categorical\n",
    "        variables (df.plot(kind-'scatter')\n",
    "\n",
    "        -   categoricals will look like vertical lines\n",
    "\n",
    "    -    Use pd.plotting.scatter_matrix to visualize possible relationships\n",
    "\n",
    "    -   ADVANCED pair-wise comparison\n",
    "        via [joint-plots](https://seaborn.pydata.org/generated/seaborn.jointplot.html)\n",
    "\n",
    "        -   ns.jointplot(x= \\<column\\>, y= \\<column\\>, data=\\<dataset\\>,\n",
    "            kind='reg')\n",
    "\n",
    "    -   **Check for linearity**\n",
    "\n",
    "4.  **Fit an intiial model**\n",
    "\n",
    "    -   Various forms, detail later...\n",
    "\n",
    "    -   **Assessing the model:**\n",
    "\n",
    "        -   Assess parameters (slope,intercept)\n",
    "\n",
    "        -   Check if the model explains the variation in the data (RMSE, F,\n",
    "            R_square)\n",
    "\n",
    "        -   *Are the coeffs, slopes, intercepts in appropriate units?*\n",
    "\n",
    "        -   *Whats the impact of collinearity? Can we ignore?*\n",
    "\n",
    "5.  **Revise the fitted model**\n",
    "\n",
    "    -   Multicollinearity is big issue for lin regression and cannot fully\n",
    "        remove it\n",
    "\n",
    "    -   Use the predictive ability of model to test it (like R2 and RMSE)\n",
    "\n",
    "    -   Check for missed non-linearity\n",
    "\n",
    "6.  **Holdout validation / Train/test split**\n",
    "\n",
    "    -   use sklearn train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "272px",
    "width": "272px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "371.475px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
